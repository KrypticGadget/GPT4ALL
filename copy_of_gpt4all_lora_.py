# -*- coding: utf-8 -*-
"""Copy of GPT4All Lora .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cGQoql_QJGR0_xZ0WbC3BBdPYWyR4xMQ
"""

!pip -q install datasets loralib sentencepiece
!pip -q install git+https://github.com/huggingface/transformers # need to install from github
!pip -q install git+https://github.com/huggingface/peft.git
!pip -q install bitsandbytes

import torch
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
import textwrap

peft_model_id = "nomic-ai/gpt4all-lora"
config = PeftConfig.from_pretrained(peft_model_id)

model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, 
                                             return_dict=True, 
                                             load_in_8bit=True, 
                                             device_map='auto')

tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)

# Load the Lora model
model = PeftModel.from_pretrained(model, peft_model_id)

def gpt4all_generate(text):
    inputs = tokenizer(
        text,
        return_tensors="pt",
    )
    input_ids = inputs["input_ids"].cuda()

    generation_config = GenerationConfig(
        temperature=0.6,
        top_p=0.95,
        repetition_penalty=1.2,
    )

    print("Generating...")
    generation_output = model.generate(
        input_ids=input_ids,
        generation_config=generation_config,
        # return_dict_in_generate=True,
        output_scores=True,
        max_new_tokens=256,
    )

    wrapped_text = textwrap.fill(tokenizer.decode(generation_output[0]), width=100)
    print(wrapped_text)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# PROMPT = """Below is an instruction that describes a task. Write a response that appropriately completes the request.
# ### Instruction:
# What are the differences between alpacas and sheep?
# ### Response:"""
# 
# gpt4all_generate(PROMPT)

PROMPT = """Below is an instruction that describes a task. Write a response that appropriately completes the request.
### Instruction:
Explain what a rainbow is.
### Response:"""
gpt4all_generate(PROMPT)

PROMPT = """Below is an instruction that describes a task. Write a response that appropriately completes the request.
\n
### Instruction:
Create a list of things I need to do to plan for my wife's birthday party.
### Response:"""
gpt4all_generate(PROMPT)

PROMPT = '''Pretend you are smart chatbot that is friendly but drunk, answer the question asked of you:
\n
###
question:
Write me an email to Sam Altman explaining that GPT-4 should be open source.
\n
###
response:
'''

gpt4all_generate(PROMPT)

PROMPT = """Below is an instruction that describes a task. Write a response that appropriately completes the request.
\n
### Instruction:
Write a rhyming limerick about a cat called Max. 
\n
### Response:"""
gpt4all_generate(PROMPT)

"""### ChatGPT & GPT-4 - limericks about Max

-1

Max was a feline of pride,  
Never one to hide,  
He strutted with grace,  
And owned every space,  
A true king of his domain, he'd decide.  

-2

Max loved to perch up high,  
Where he could watch the world go by,  
From his lofty view,  
He'd contemplate what to do,  
And sometimes let out a contented sigh.  

-3 GPT-4

There once was a cat named Max  
Whose paws had incredible attacks  
He'd leap and he'd pounce  
On a mouse in the house  
And no prey could ever relax  
"""

PROMPT = """Below is an instruction that describes a task. Write a response that appropriately completes the request.
\n
### Instruction:
Write a python function the detects if a number is a prime number or not.
\n
### Response:"""
gpt4all_generate(PROMPT)

def is_prime(num):     
    for i in range(2, num + 1):         
        if (num % i) == 0:  
            return False     
        else:         
            return True

is_prime(15) #

